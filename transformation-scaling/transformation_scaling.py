# -*- coding: utf-8 -*-
"""transformation-scaling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11bh_JIBkupgr918sSny-5crZwGzqnOXB
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler, RobustScaler, Normalizer

df =pd.read_csv('mtcars_mod.csv')
df

"""# Which scaling method works best for data with outliers?

- Robust Scaler

# Which scaling method produces data that is normally distributed? What is its mean and variance?


- StandardScaler transforms the data to have a mean of 0 and a variance of 1

# Which scaling method does not remove sparsity? What is sparsity?
- MaxAbsScaler does not remove sparsity.
-  Sparsity refers to the percentage of zero values in a dataset.

# Which scaling method is best to use if the bounds of your data are known from domain knowledge?

- MinMaxScaler

# Use the most appropriate scaling technique for each variable to scale disp, hp, drat, and wt, assuming wt should be normally distributed and the true bounds are known for disp.
"""

fig, ax = plt.subplots(2,2, figsize=(10,5))
sns.kdeplot(df['disp'], ax = ax[0,0]);
sns.kdeplot(df['hp'],ax = ax[0,1]);
sns.kdeplot(df['drat'], ax = ax[1,0]);
sns.kdeplot(df['wt'], ax = ax[1,1]);
fig.tight_layout()

# min_max_scaler as the true bounds were known as disp
min_max_scaler = MinMaxScaler()
min_max_scaler.fit(df[['disp']])
df['disp_mms'] = min_max_scaler.transform(df[['disp']])

fig, ax = plt.subplots(1,2, figsize=(10,5))
sns.kdeplot(df['disp'], ax = ax[0]);
sns.kdeplot(df['disp_mms'], ax = ax[1]);

#hp contains a lot of 0
max_abs_scaler = MaxAbsScaler()
max_abs_scaler.fit(df[['hp']])
df['hp_mas'] = max_abs_scaler.transform(df[['hp']])

fig, ax = plt.subplots(1,2, figsize=(10,5))
sns.kdeplot(df['hp'], ax = ax[0]);
sns.kdeplot(df['hp_mas'], ax = ax[1]);

# robust scaler for drat having outliers
robust_scaler = RobustScaler()
robust_scaler.fit(df[['drat']])
df['drat_rs'] = robust_scaler.transform(df[['drat']])

fig, ax = plt.subplots(1,2, figsize=(10,5))
sns.kdeplot(df['drat'], ax = ax[0]);
sns.kdeplot(df['drat_rs'], ax = ax[1]);

# Standard scaler for wt being normally distributed
standard_scaler = StandardScaler()
standard_scaler.fit(df[['wt']])
df['wt_std'] = standard_scaler.transform(df[['wt']])

fig, ax = plt.subplots(1,2, figsize=(10,5))
sns.kdeplot(df['wt'], ax = ax[0]);
sns.kdeplot(df['wt_std'], ax = ax[1]);

"""# Use Box-Cox or Yeo-Johnson to determine the best transformation to use to make mpg and qsec normally distributed. Show the transformed data and the value of $\lambda$."""

from scipy.stats import boxcox, yeojohnson

fig, ax = plt.subplots(1,2, figsize=(10,5))
sns.kdeplot(df['mpg'], ax = ax[0]);
sns.kdeplot(df['qsec'], ax = ax[1]);

df.mpg.skew() # right skewed

df.qsec.skew() #left skewed

# box cox for mpg
mpg_bc, lmbd = boxcox(df.mpg)
df['mpg_bc'] = mpg_bc
sns.kdeplot(df.mpg_bc);#

lmbd #lambda

df.mpg_bc.skew() # more normally distributed

# Yeo-Johnson for qsec having negative data
qsec_yj, lmbd = yeojohnson(df.qsec)
df['qsec_yj'] = qsec_yj
sns.kdeplot(df.qsec_yj);

lmbd

df.qsec_yj.skew() # a little bit better than original still not normally distributed

"""# Normalize all of the numeric rows using L2-normalization and display the new dataframe."""

df.describe()

norm = Normalizer() # default to norm = 'l2'

numeric_cols = df.select_dtypes(include=['number']).columns
numeric_cols

norm.fit(df[numeric_cols])

data = norm.transform(df[numeric_cols])
data

[(elem[0]**2 + elem[1]**2) for elem in data] # it meets criteria