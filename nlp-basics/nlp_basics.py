# -*- coding: utf-8 -*-
"""nlp-basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-P7ocqgYS2b_Z3p8ugxsEGuhNXKRb2Aq

# Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
!pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv('winemag-data.csv')
data

"""## Tokenize the description feature"""

nltk.download('punkt')
nltk.download('punkt_tab')

description = data['description']

tok_desc = []
for desc in description:
  words = word_tokenize(desc)
  tok_desc.append(words)

data['nlp_description'] = tok_desc

data['nlp_description']

data['nlp_description'][0]

"""## Remove stop words"""

nltk.download('stopwords')

stop_words = stopwords.words('english')
stop_words

stop_words.extend(['.', ',', '!', '?'])

desc_nosw = []
for desc_list in data['nlp_description']: # Iterate through lists of words
    new_desc = []
    for word in desc_list:
        if word.lower() not in stop_words:
            new_desc.append(word)
    desc_nosw.append(new_desc)

data['nlp_description'] = desc_nosw

print(data[['description', 'nlp_description']].head())

data['nlp_description'][0]

"""Stem the tokens"""

ps = PorterStemmer()

stemmed_tokens = []
for desc_list in data['nlp_description']:
    stemmed_desc = []
    for word in desc_list:
        stemmed_word = ps.stem(word)
        stemmed_desc.append(stemmed_word)
    stemmed_tokens.append(" ".join(stemmed_desc))


data['Cleaned_Stem_Description'] = stemmed_tokens
print(data[['description', 'Cleaned_Stem_Description']].head())

"""Lemmatize the tokens"""

nltk.download('wordnet')
wnl = WordNetLemmatizer()

lemmatized_tokens = []
for desc_list in data['nlp_description']:
    lemmatized_desc = []
    for word in desc_list:
        lemmatized_word = wnl.lemmatize(word)
        lemmatized_desc.append(lemmatized_word)
    lemmatized_tokens.append(" ".join(lemmatized_desc))

data['Cleaned_Lemma_Description'] = lemmatized_tokens
print(data[['description', 'Cleaned_Lemma_Description']].head())

"""Build a word cloud based on Cleaned_Lemma_Description"""

type(data['Cleaned_Lemma_Description'])

text = " ".join(data['Cleaned_Lemma_Description'])
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud)

"""## Sentiment Analysis"""

from textblob import TextBlob
!pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

data['TextBlob_Polarity'] = data['Cleaned_Lemma_Description'].apply(lambda x: TextBlob(x).sentiment.polarity)
data['Vader_Compound'] = data['Cleaned_Lemma_Description'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

data[['TextBlob_Polarity', 'Vader_Compound']]

data['Review_TB'] = data['TextBlob_Polarity'].apply(lambda x: 1 if x > 0.333 else -1 if x < -0.333 else 0)

data['Review_TB']

data['Review_Vader'] = data['Vader_Compound'].apply(lambda x: 1 if x > 0.333 else -1 if x < 0.333 else 0)

data['Review_Vader']

data['Cleaned_Lemma_Description'][129968]

data