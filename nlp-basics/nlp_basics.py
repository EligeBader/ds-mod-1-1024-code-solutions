# -*- coding: utf-8 -*-
"""nlp-basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-P7ocqgYS2b_Z3p8ugxsEGuhNXKRb2Aq

# Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
!pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

data = pd.read_csv('winemag-data.csv')
data

"""## Tokenize the description feature"""

nltk.download('punkt')
nltk.download('punkt_tab')

description = data['description']

tok_desc = []
for desc in description:
  words = word_tokenize(desc)
  tok_desc.append(words)

data['nlp_description'] = tok_desc

data['nlp_description']

data['nlp_description'][0]

"""## Remove stop words"""

nltk.download('stopwords')

stop_words = stopwords.words('english')
stop_words

stop_words.extend(['.', ',', '!', '?'])

desc_nosw = []
for desc_list in data['nlp_description']: # Iterate through lists of words
    new_desc = []
    for word in desc_list:
        if word.lower() not in stop_words:
            new_desc.append(word)
    desc_nosw.append(new_desc)

data['nlp_description'] = desc_nosw

print(data[['description', 'nlp_description']].head())

data['nlp_description'][0]

"""Stem the tokens"""

ps = PorterStemmer()

stemmed_tokens = []
for desc_list in data['nlp_description']:
    stemmed_desc = []
    for word in desc_list:
        stemmed_word = ps.stem(word)
        stemmed_desc.append(stemmed_word)
    stemmed_tokens.append(stemmed_desc)

data['nlp_description'] = stemmed_tokens
print(data[['description', 'nlp_description']].head())

"""Lemmatize the tokens"""

nltk.download('wordnet')
wnl = WordNetLemmatizer()

lemmatized_tokens = []
for desc_list in data['nlp_description']:
    lemmatized_desc = []
    for word in desc_list:
        lemmatized_word = wnl.lemmatize(word)
        lemmatized_desc.append(lemmatized_word)
    lemmatized_tokens.append(lemmatized_desc)

data['nlp_description'] = lemmatized_tokens
print(data[['description', 'nlp_description']].head())